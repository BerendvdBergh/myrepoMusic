---
title: "Berend's portfolio, homework 8 version"
author: "Berend van den Bergh"
date: "Block 4"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: lumen
    self_contained: true
    orientation: columns

---

```{r setup, include=FALSE}
library(tidyverse)
library(flexdashboard)
library(plotly)
```

### Welcome to my github page on musicology!


Dear reader, I welcome you to my github page. On the other tabs you will find some visualisations of the (soon to be class corpus of music) 2024 AI Song contest. 

For this portfolio I have created two songs using AI. The prompts for these songs can be found on the next page. The songs i have created using AI were inspired by some of these songs below. Now, I would like to give you a small challenge: either have a listen to these songs first and then come up with a prompt, to try to make a similar song to the songs below. After this, have a look at the prompts I have used to try and create similar songs. Did these match your idea of the best prompt?

Alternatively, you can take a look at my prompts first, then imagine what this type of music would sound like and then listen to the songs below. 

Whichever route you took, be sure to listen to the final result of the AI songs generated by me. Do you think they fit the prompts and/or the songs?

(Currently the embedded links are not working properly, because using them stops the website from appearing as a storyboard. If you still wish to hear what the songs are like you can look up these titles below.)

```{r, results='asis', echo=FALSE}
cat('<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6iGMRy5RDMiAdTpuMB5gAm?utm_source=generator" width="100%" height="80" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>')

cat('<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2I7Ph7hRkjOMgy9WCsE4F3?utm_source=generator" width="100%" height="80" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>')

cat('<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1C5mmdbWD5ksvTU0gHPfao?utm_source=generator" width="100%" height="80" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>')
```

Here are my AI generated songs. They were created using [StableAudio](https://www.stableaudio.com/)
(songs yet to be added)

<details>
  <summary style="color: blue; text-decoration: underline; cursor: pointer;">Show prompts</summary>
  
The prompt for the first song was: *"A house/hardgroove beat with high energy and danceability, a blend of hefty kicks with catchy riffs and stabs, with a BPM of 143, the key should be in C# Major and the camelot should be 3B."*

The prompt for the second song was: *"A hardhouse/trance beat with high energy and danceability, with a bpm of 150, the key should be in F# Minor and the camelot should be 11A. It should have a strong basskick."*

I came up with these prompts by looking up descriptions of the artists of the songs that I tried to recreate, and copying how their styles were described. I also looked up these songs on [Tunebat](https://tunebat.com/), which is a website that shows lots of musical data on any song via the spotify API.

### Visualising the class corpus and my own genAI tracks

```{r}
classcorp <- read_csv("C:/Users/beren/myrepoMusic/compmus2025.csv")

classcorp <- classcorp |> 
  mutate(highlight = ifelse(filename %in% c("berend-b-1", "berend-b-2"), "yes", "no"))

ggplot(classcorp, aes(
    x = tempo,
    y = arousal,
    size = instrumentalness,
    colour = danceability
  )) +


  geom_point(alpha = 0.7, position = position_jitter(width = 0, height = 0.2)) +              # Scatter plot.
  geom_rug(linewidth = 0.2) + # Add 'fringes' to show data distribution.
 
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(70, 150),
    breaks = c(80, 100, 120, 140), # Specify grid lines
    minor_breaks = NULL       # Remove 'minor' grid lines.
  ) +
  
   # Highlighting your own songs
  geom_point(
    #data = classcorp |> filter(filename %in% c("berend-b-1", "berend-b-2")),
    data = classcorp |> filter(highlight == "yes")

    #aes(x = tempo, y = arousal, colour = danceability, size = instrumentalness)
    
  ) +
  geom_text(
    data = classcorp |> filter(filename %in% c("berend-b-1", "berend-b-2")),
    aes(label = filename),
  hjust = -2.2,   # Shift slightly to the left
  vjust = -2.2,   # Move label above the point
  size = 3,        # Keep label readable but not too big
  color = "red"
  ) +
  
    # Adding annotation about size
  annotate(
    "text", 
    x = 140,  
    y = 6.8,  
    label = "Size = Instrumentalness", 
    size = 3, 
    hjust = 1,  
    color = "black"
  ) +
  
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(3, 7),
    breaks = c(4, 5, 6),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c() +  # Use the popular viridis colour palette.
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud..
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Tempo",
    y = "Arousal",
    colour = "Danceability"
  )
ggplotly()

# ------




```


***

This is my visualisation of the class corpus. 

### Conclusion and discussion!

I did not have too much time this week yet, but as one can see from the visualisations it is clear that my songs have very high dancebility and arousal in comparison to the class corpus, with an average tempo and low instrumentalness
